# ğŸš• Taxi-v3 Reinforcement Learning Agent

A Q-Learning agent trained on OpenAI Gymnasium's Taxi-v3 environment.
Includes a fully playable browser-based game UI where you can watch 
the trained agent navigate, or take control yourself.

---

## ğŸ§  What is this?

The Taxi-v3 environment is a 5Ã—5 grid where a taxi must:
1. Navigate to a passenger's location
2. Pick them up
3. Drop them off at the correct destination

The agent learns this entirely through trial and error using 
**Q-Learning** â€” a model-free reinforcement learning algorithm.
After 10,000 episodes it consistently solves the environment 
in under 15 steps with a positive reward.

---

## ğŸ“ Project Structure

| File | Description |
|------|-------------|
| `taxi_rl.ipynb` | Full training notebook â€” run on Google Colab |
| `taxi_rl_train.py` | Standalone Python training script |
| `taxi_rl_game.html` | Browser game UI â€” open locally or via GitHub Pages |

---

## ğŸš€ How to Run

### Train the Agent (Google Colab â€” no install needed)
1. Open `taxi_rl.ipynb` in [Google Colab](https://colab.research.google.com)
2. Click **Runtime â†’ Run All**
3. Training completes in ~2 minutes
4. Download the generated `q_table.json`

### Play the Game
1. Open `taxi_rl_game.html` in any browser
2. The game runs immediately with a built-in agent
3. To use your trained agent: click the file picker and load `q_table.json`
4. Switch between **Agent mode** (watch it play) and **Manual mode** (play yourself)

---

## ğŸ® Game Controls

| Key | Action |
|-----|--------|
| â†‘ â†“ â† â†’ | Move taxi |
| P | Pick up passenger |
| D | Drop off passenger |

---

## ğŸ“Š Algorithm

**Q-Learning** with Îµ-greedy exploration.

| Parameter | Value |
|-----------|-------|
| Episodes | 10,000 |
| Learning rate (Î±) | 0.10 |
| Discount factor (Î³) | 0.99 |
| Epsilon start | 1.0 |
| Epsilon end | 0.01 |
| Epsilon decay | 0.0005 |

The Q-table has **500 states Ã— 6 actions** = 3,000 values,
updated each step using the Bellman equation:
```
Q(s,a) â† Q(s,a) + Î± [ r + Î³ Â· max Q(s',a') âˆ’ Q(s,a) ]
```

---

## ğŸ“ˆ Results

After training:
- âœ… Average reward: **+8 to +9** (random agent scores ~âˆ’200)
- âœ… Average steps to solve: **~13**
- âœ… Success rate: **~98%** over 500 evaluation episodes

---

## ğŸ›  Tech Stack

- Python 3.10+
- [Gymnasium](https://gymnasium.farama.org/) â€” environment
- NumPy â€” Q-table operations
- Matplotlib â€” training curves
- Vanilla HTML / CSS / JS â€” game UI (zero dependencies)

---

## ğŸ“Œ What I Learned

- How Q-Learning and the Bellman equation work in practice
- The exploration vs exploitation tradeoff (Îµ-greedy)
- How to decode Gymnasium's state encoding
- Building an RL environment from scratch in JavaScript for the UI

---

## ğŸ”® Future Improvements

- [ ] Replace Q-table with a Deep Q-Network (DQN) using PyTorch
- [ ] Add live reward and epsilon decay graphs in the UI
- [ ] Visualize the Q-table as a heatmap
- [ ] Try harder environments (LunarLander, CartPole)
