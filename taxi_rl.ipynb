{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš• Taxi-v3 Reinforcement Learning â€” Q-Learning\n",
    "\n",
    "This notebook trains a Q-Learning agent on the classic `Taxi-v3` environment, then exports the Q-table so you can load it into the interactive web UI.\n",
    "\n",
    "**Steps:**\n",
    "1. Install deps\n",
    "2. Train the agent\n",
    "3. Evaluate & visualize\n",
    "4. Export `q_table.json` for the game UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print('âœ… Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Â· Environment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state, _ = env.reset()\n",
    "\n",
    "print(f'Observation space : {env.observation_space}  ({env.observation_space.n} states)')\n",
    "print(f'Action space      : {env.action_space}  ({env.action_space.n} actions)')\n",
    "print()\n",
    "print('Actions: 0=South  1=North  2=East  3=West  4=Pickup  5=Dropoff')\n",
    "print()\n",
    "print('Initial state render:')\n",
    "print(env.render())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Â· Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feel free to tweak these â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EPISODES       = 10_000   # Training episodes\n",
    "ALPHA          = 0.10     # Learning rate\n",
    "GAMMA          = 0.99     # Discount factor\n",
    "EPSILON_START  = 1.0      # Initial exploration rate\n",
    "EPSILON_END    = 0.01     # Minimum exploration rate\n",
    "EPSILON_DECAY  = 0.0005   # Linear decay per episode\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(f'Training for {EPISODES:,} episodes')\n",
    "print(f'Î±={ALPHA}  Î³={GAMMA}  Îµ: {EPSILON_START}â†’{EPSILON_END} (decay={EPSILON_DECAY})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Â· Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "n_states  = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "epsilon = EPSILON_START\n",
    "rewards_history = []\n",
    "t_start = time.time()\n",
    "\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = int(np.argmax(Q[state]))\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Bellman update\n",
    "        Q[state, action] += ALPHA * (\n",
    "            reward + GAMMA * np.max(Q[next_state]) - Q[state, action]\n",
    "        )\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
    "    rewards_history.append(total_reward)\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        avg = np.mean(rewards_history[-1000:])\n",
    "        elapsed = time.time() - t_start\n",
    "        print(f'Ep {episode:>6} | Avg reward (1k): {avg:+7.2f} | Îµ={epsilon:.4f} | {elapsed:.0f}s')\n",
    "\n",
    "env.close()\n",
    "print(f'\\nâœ… Training done in {time.time()-t_start:.1f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Â· Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(data, window=100):\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "fig.patch.set_facecolor('#0a0a0f')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_facecolor('#12121a')\n",
    "    ax.tick_params(colors='#6b6b8a')\n",
    "    ax.spines[:].set_color('#1e1e2e')\n",
    "\n",
    "# Raw rewards\n",
    "axes[0].plot(rewards_history, alpha=.2, color='#4dabf7', linewidth=.5)\n",
    "axes[0].plot(moving_avg(rewards_history, 200), color='#f5c518', linewidth=1.5, label='MA-200')\n",
    "axes[0].set_title('Reward per Episode', color='#e0e0f0')\n",
    "axes[0].set_xlabel('Episode', color='#6b6b8a')\n",
    "axes[0].set_ylabel('Total Reward', color='#6b6b8a')\n",
    "axes[0].legend(facecolor='#12121a', labelcolor='#e0e0f0')\n",
    "\n",
    "# Avg reward per 1000 episodes\n",
    "chunk = 1000\n",
    "chunked = [np.mean(rewards_history[i:i+chunk]) for i in range(0, len(rewards_history)-chunk+1, chunk)]\n",
    "axes[1].bar(range(len(chunked)), chunked, color='#39d353', alpha=.7)\n",
    "axes[1].set_title('Avg Reward per 1k Episodes', color='#e0e0f0')\n",
    "axes[1].set_xlabel('1k Episode Block', color='#6b6b8a')\n",
    "axes[1].set_ylabel('Avg Reward', color='#6b6b8a')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curve.png', dpi=120, bbox_inches='tight', facecolor='#0a0a0f')\n",
    "plt.show()\n",
    "print('Chart saved as training_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Â· Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('Taxi-v3')\n",
    "eval_rewards, eval_steps = [], []\n",
    "\n",
    "N_EVAL = 500\n",
    "for _ in range(N_EVAL):\n",
    "    s, _ = eval_env.reset()\n",
    "    total, n_steps, done = 0, 0, False\n",
    "    while not done:\n",
    "        action = int(np.argmax(Q[s]))\n",
    "        s, r, term, trunc, _ = eval_env.step(action)\n",
    "        done = term or trunc\n",
    "        total += r; n_steps += 1\n",
    "    eval_rewards.append(total)\n",
    "    eval_steps.append(n_steps)\n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "print(f'=== Evaluation over {N_EVAL} episodes ===')\n",
    "print(f'Avg reward : {np.mean(eval_rewards):+.2f}')\n",
    "print(f'Avg steps  : {np.mean(eval_steps):.1f}')\n",
    "print(f'Success (reward>0): {100*np.mean(np.array(eval_rewards)>0):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Â· Watch Agent Play (text render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state, _ = render_env.reset()\n",
    "done = False\n",
    "ACTION_NAMES = ['â¬‡ South','â¬† North','â¡ East','â¬… West','ğŸ§ Pickup','ğŸ“ Dropoff']\n",
    "\n",
    "for step in range(50):\n",
    "    clear_output(wait=True)\n",
    "    print(render_env.render())\n",
    "    if done: print('âœ… Done!'); break\n",
    "    action = int(np.argmax(Q[state]))\n",
    "    state, reward, terminated, truncated, _ = render_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(f'Step {step+1}: {ACTION_NAMES[action]}  â†’  reward={reward:+d}')\n",
    "    time.sleep(0.4)\n",
    "\n",
    "render_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Â· Export Q-table â†’ Load into Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q_table.json', 'w') as f:\n",
    "    json.dump(Q.tolist(), f)\n",
    "\n",
    "print('âœ… q_table.json saved!')\n",
    "print(f'   Shape: {Q.shape}  ({Q.shape[0]} states Ã— {Q.shape[1]} actions)')\n",
    "print()\n",
    "print('Next step:')\n",
    "print('  1. Open  taxi_rl_game.html  in your browser')\n",
    "print('  2. Click the file picker in the Agent panel')\n",
    "print('  3. Select  q_table.json  â€” your trained agent will take over!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
